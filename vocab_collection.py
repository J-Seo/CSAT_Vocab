# -*- coding: utf-8 -*-
"""Vocab_collection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nBevCYbiu3MXI7G1S9UJqBw6_RseFq95
"""

!pip install docx2txt
# !pip install -q tensorflow-gpu==2.0.0-beta0

from __future__ import absolute_import, division, print_function, unicode_literals
from nltk import word_tokenize
from nltk.corpus import stopwords
from nltk.corpus import wordnet as wn
from urllib.request import Request, urlopen
from bs4 import BeautifulSoup
from nltk.stem import WordNetLemmatizer


# import tensorflow as tf
import nltk, re, pprint
import docx2txt
import numpy as np
import os

# download 
nltk.download('words')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# 1. 구글 드라이브에섯 기출 데이터셋 가져오기

from google.colab import drive
drive.mount('/content/gdrive')

# 2. 수능 기출 데이터 불러오기
# 3. 해당 경로의 데이터셋 열기 + 읽기 + utf-8로 해독하기

## text = open(f, 'rb').read().decode(encoding = 'utf-8')


# txt 파일 불러오기

#Jaehyung_sp = open('/content/gdrive/My Drive/Colab Notebooks/Jaehyung_sp', 'rb').read().decode(encoding = 'utf-8')
#Minhyung_or = open('/content/gdrive/My Drive/Colab Notebooks/Minhyung_or', 'rb').read().decode(encoding = 'utf-8')
#Minhyung_or2 = open('/content/gdrive/My Drive/Colab Notebooks/Minhyung_or2', 'rb').read().decode(encoding = 'utf-8')
#Minhyung_or3 = open('/content/gdrive/My Drive/Colab Notebooks/Minhyung_or3', 'rb').read().decode(encoding = 'utf-8')
#Minhyung_or4 = open('/content/gdrive/My Drive/Colab Notebooks/Minhyung_or4', 'rb').read().decode(encoding = 'utf-8')

# docx 파일 불러오기

#Kanghee_at = docx2txt.process('/content/gdrive/My Drive/Colab Notebooks/Kanghee_at.docx')
#Kanghee_ct = docx2txt.process('/content/gdrive/My Drive/Colab Notebooks/Kanghee_ct.docx')

file_names =['Donghwan_vg', 'Donghwan_bk', 'Jaehyung_sp.dms', 'Kanghee_at', 'Kanghee_ct', 'Minhyeong.dms' ]
file_path = []

# 3. 구글 드라이브에서 파일들의 경로 리스트

def load_file_path(file_names):
  for file in file_names:
    text = '/content/gdrive/My Drive/Colab Notebooks/{}'.format(file)
    file_path.append(text)
  return file_path

file_path = load_file_path(file_names)
print(file_path)  
#text1 = '/content/gdrive/My Drive/Colab Notebooks/Donghwan_bk'
#text2= '/content/gdrive/My Drive/Colab Notebooks/Minhyung_or'
#text3 = '/content/gdrive/My Drive/Colab Notebooks/Minhyung_or2'
#text4 = '/content/gdrive/My Drive/Colab Notebooks/Minhyung_or3'
#text5 = '/content/gdrive/My Drive/Colab Notebooks/Minhyung_or4'
#text6 = '/content/gdrive/My Drive/Colab Notebooks/'
## docx 파일은 합치는것이 가능하나 복잡한 상황임 

#text6 = '/content/gdrive/My Drive/Colab Notebooks/Kanghee_at.docx'
#text7 = '/content/gdrive/My Drive/Colab Notebooks/Kanghee_ct.docx'

#filenames_txt = [text1, text2, text3, text4, text5]
#filenames_docx = [Kanghee_at, Kanghee_ct]

# 4. 텍스트 파일 합치기

def merge_txt(file_path):
  filenames = file_path
  with open('/content/gdrive/My Drive/Colab Notebooks/merge2.txt', 'w') as outfile:
      for fname in filenames:
          with open(fname) as infile:
              for line in infile:
                  outfile.write(line)
                  
  return outfile

merge_txt(file_path)

# 5. 합쳐진 텍스트 불러오기

merge_text = open('/content/gdrive/My Drive/Colab Notebooks/merge2.txt', 'rb').read().decode(encoding = 'utf-8')

# 6. Token화 하기

tokens = word_tokenize(merge_text)
print(type(merge_text))
print(len(merge_text))

# 7. 특수 문자, 숫자 제거, 중복 단어 제거

word_list = sorted(set(words.lower() for words in tokens if words.isalpha()))
print(type(word_list))
print(len(word_list))

# 8. 자주 사용하지 않은 단어 제거

def unusual_words(text):
    text_vocab = set(w.lower() for w in text if w.isalpha())
    english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    unusual = text_vocab - english_vocab
    return sorted(unusual)
  
word_list = unusual_words(word_list) 
print(type(word_list))
print(len(word_list))

# 9. Stopwords 제거  

def content_word(text):
  stopwords = nltk.corpus.stopwords.words('english')
  content = [w for w in text if w.lower() not in stopwords]
  return content


word_list = content_word(word_list)
print(type(word_list))
print(len(word_list))

# 10. 워드넷 적용해보기

# 반의어
##>>> wn.lemma('supply.n.02.supply').antonyms()
##[Lemma('demand.n.02.demand')]
##>>> wn.lemma('rush.v.01.rush').antonyms()
##[Lemma('linger.v.04.linger')]
##>>> wn.lemma('horizontal.a.01.horizontal').antonyms()
##[Lemma('inclined.a.02.inclined'), Lemma('vertical.a.01.vertical')]
##>>> wn.lemma('staccato.r.01.staccato').antonyms()
##[Lemma('legato.r.01.legato')]

# 11. BS4로 크롤링하기 (연습)

syn_list = []
basic_url="https://www.merriam-webster.com/thesaurus/create"

def crawling_dict(basic_url):
  req = Request(basic_url)
  res_test = urlopen(req)
  html = res_test.read().decode('utf-8') # 한국어일 경우 'cp949'

  bs = BeautifulSoup(html, 'html.parser')
  tags = bs.findAll('span', {'class' : "thes-list rel-list" })

  for tag in tags:
    syn = tag.get_text()
    syn = sorted(set(syn.split()[4:]))
    for word in syn:
      if len(word) > 3 and not '(' in word:
        syn_list.append(word)
  return syn_list

# 12. 단어 원형 복원하기 

def lemmatize(word_list):
  lm = WordNetLemmatizer()
  [lm.lemmatize(w) for w in word_list]
  return word_list
  
word_list = lemmatize(word_list)

# 13. 유사어 사전 만들기
## {'Vocab1' : ['synonym1', 'synonym2', ... , 'synonymN'], 'Vocab2'}


def synonym_dict(word_list):
  CSAT_dict = {}
  for words in word_list:
    syn_list = []
    url = "https://www.merriam-webster.com/thesaurus/{}".format(words)
    try:
      req = Request(url)
      res = urlopen(req)
      html = res.read().decode('utf-8')
  
      bs = BeautifulSoup(html, 'html.parser')
      tags = bs.findAll('span', {'class' : "thes-list rel-list"})
  
      for tag in tags:
        syn = tag.get_text()
        syn = sorted(set(syn.split()[4:]))
        for word in syn:
          if len(word) > 3 and not '(' in word:
            syn_list.append(word)
      CSAT_dict[words] = syn_list
    
    except:
      print('{}는 사전에 없는 단어입니다'.format(words))
    
    
  return CSAT_dict

CSAT_dict = synonym_dict(word_list)

# 14. 반의어 사전 만들기
def antonym_dict(word_list):
  CSAT_ant_dict = {}
  for syn in word_list:
    name_list = []
    syns = wn.synsets(syn)
    names = [s.name() for s in syns]
    for name in names:
      if syn in name:
        name_list.append(name)
      try:
        for n in name_list:
          stem_name = n.split('.')[0]
          atn = wn.lemma('{}.{}'.format(n, stem_name)).antonyms()
          if atn is not []:
            print('{}의 반의어는 {}'.format(stem_name, atn))
            CSAT_ant_dict[stem_name] = ant
      except:
        pass
  return CSAT_ant_dict

antonym_dict(word_list)







